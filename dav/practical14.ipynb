{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ac9381",
   "metadata": {},
   "source": [
    "# Bias-Variance Decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "228e9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85a57107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing, load_iris, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529dcd61",
   "metadata": {},
   "source": [
    "## Define Bias-Variance Decomposition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f10714f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_decomp(model, X_train, y_train, X_test, y_test, n_bootstraps=50):\n",
    "    \"\"\"Simple bias-variance decomposition for regression\"\"\"\n",
    "    # Array to store predictions from each bootstrap model\n",
    "    predictions = np.zeros((n_bootstraps, len(y_test)))\n",
    "    \n",
    "    # For each bootstrap sample\n",
    "    for i in range(n_bootstraps):\n",
    "        # Create bootstrap sample\n",
    "        X_boot, y_boot = resample(X_train, y_train, random_state=i)\n",
    "        \n",
    "        # Train model on bootstrap sample\n",
    "        model.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predict on test data\n",
    "        predictions[i] = model.predict(X_test)\n",
    "    \n",
    "    # Average prediction across all bootstrap models\n",
    "    average_pred = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Calculate squared bias\n",
    "    bias_squared = np.mean((average_pred - y_test) ** 2)\n",
    "    \n",
    "    # Calculate variance\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    # Calculate average error\n",
    "    error = np.mean(np.mean((predictions - y_test.reshape(1, -1)) ** 2, axis=0))\n",
    "    \n",
    "    # Noise (irreducible error)\n",
    "    noise = error - bias_squared - variance\n",
    "    \n",
    "    return bias_squared, variance, error, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424b69c",
   "metadata": {},
   "source": [
    "## 1. California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92fbe59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing Dataset: (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X_california = california.data\n",
    "y_california = california.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_california_scaled = scaler.fit_transform(X_california)\n",
    "\n",
    "# Split data\n",
    "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(\n",
    "    X_california_scaled, y_california, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"California Housing Dataset: {X_california.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06aa50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing Dataset Results:\n",
      "------------------------------------------------------------\n",
      "Max Depth  Bias²           Variance        Total Error    \n",
      "------------------------------------------------------------\n",
      "1          0.9177          0.0283          0.9460         \n",
      "1          0.9177          0.0283          0.9460         \n",
      "3          0.5990          0.0482          0.6472         \n",
      "3          0.5990          0.0482          0.6472         \n",
      "5          0.4615          0.0712          0.5327         \n",
      "5          0.4615          0.0712          0.5327         \n",
      "10         0.2972          0.1586          0.4558         \n",
      "10         0.2972          0.1586          0.4558         \n",
      "None       0.2548          0.2999          0.5547         \n",
      "------------------------------------------------------------\n",
      "None       0.2548          0.2999          0.5547         \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define different complexity levels for decision trees\n",
    "max_depths = [1, 3, 5, 10, None]\n",
    "\n",
    "# Store results\n",
    "bias_values_cal = []\n",
    "variance_values_cal = []\n",
    "error_values_cal = []\n",
    "\n",
    "# Perform bias-variance decomposition\n",
    "print(\"California Housing Dataset Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Max Depth':<10} {'Bias²':<15} {'Variance':<15} {'Total Error':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    bias, variance, error, noise = bias_variance_decomp(\n",
    "        model, X_train_cal, y_train_cal, X_test_cal, y_test_cal\n",
    "    )\n",
    "    \n",
    "    bias_values_cal.append(bias)\n",
    "    variance_values_cal.append(variance)\n",
    "    error_values_cal.append(error)\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else \"None\"\n",
    "    print(f\"{depth_str:<10} {bias:<15.4f} {variance:<15.4f} {error:<15.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04cae5",
   "metadata": {},
   "source": [
    "## 2. Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e76d1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified bias-variance decomposition for classification\n",
    "def bias_variance_decomp_clf(model, X_train, y_train, X_test, y_test, n_bootstraps=50):\n",
    "    \"\"\"Simplified bias-variance estimation for classification\"\"\"\n",
    "    predictions = np.zeros((n_bootstraps, len(y_test)))\n",
    "    \n",
    "    # For each bootstrap sample\n",
    "    for i in range(n_bootstraps):\n",
    "        # Create bootstrap sample\n",
    "        X_boot, y_boot = resample(X_train, y_train, random_state=i)\n",
    "        \n",
    "        # Train model on bootstrap sample\n",
    "        model.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predict on test data\n",
    "        predictions[i] = model.predict(X_test)\n",
    "    \n",
    "    # Mode prediction (most common class) for each test point\n",
    "    from scipy import stats\n",
    "    main_predictions = stats.mode(predictions, axis=0, keepdims=False)[0]\n",
    "    \n",
    "    # Bias - error between main prediction and true class\n",
    "    bias = np.mean(main_predictions != y_test)\n",
    "    \n",
    "    # Variance - disagreement between individual models\n",
    "    variance = np.mean([np.mean(pred != main_predictions) for pred in predictions])\n",
    "    \n",
    "    # Total error - average misclassification rate\n",
    "    error = np.mean([np.mean(pred != y_test) for pred in predictions])\n",
    "    \n",
    "    return bias, variance, error, 0  # Noise is 0 for this simplified approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51e6a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Split data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris_scaled, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"Iris Dataset: {X_iris.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04859eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Results:\n",
      "------------------------------------------------------------\n",
      "Max Depth  Bias            Variance        Total Error    \n",
      "------------------------------------------------------------\n",
      "1          0.1000          0.3367          0.3440         \n",
      "2          0.0667          0.0347          0.0720         \n",
      "3          0.0333          0.0287          0.0620         \n",
      "3          0.0333          0.0287          0.0620         \n",
      "5          0.0667          0.0367          0.0727         \n",
      "None       0.0667          0.0367          0.0727         \n",
      "------------------------------------------------------------\n",
      "5          0.0667          0.0367          0.0727         \n",
      "None       0.0667          0.0367          0.0727         \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define different complexity levels for decision trees\n",
    "max_depths_iris = [1, 2, 3, 5, None]\n",
    "\n",
    "# Store results\n",
    "bias_values_iris = []\n",
    "variance_values_iris = []\n",
    "error_values_iris = []\n",
    "\n",
    "# Perform bias-variance decomposition\n",
    "print(\"Iris Dataset Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Max Depth':<10} {'Bias':<15} {'Variance':<15} {'Total Error':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for depth in max_depths_iris:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    bias, variance, error, _ = bias_variance_decomp_clf(\n",
    "        model, X_train_iris, y_train_iris, X_test_iris, y_test_iris\n",
    "    )\n",
    "    \n",
    "    bias_values_iris.append(bias)\n",
    "    variance_values_iris.append(variance)\n",
    "    error_values_iris.append(error)\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else \"None\"\n",
    "    print(f\"{depth_str:<10} {bias:<15.4f} {variance:<15.4f} {error:<15.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951ab95",
   "metadata": {},
   "source": [
    "## 3. Random Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96a088f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Dataset: (500, 10)\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X_random, y_random = make_regression(\n",
    "    n_samples=500, n_features=10, n_informative=5, noise=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train_random, X_test_random, y_train_random, y_test_random = train_test_split(\n",
    "    X_random, y_random, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Random Dataset: {X_random.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b3fce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Dataset Results:\n",
      "------------------------------------------------------------\n",
      "Max Depth  Bias²           Variance        Total Error    \n",
      "------------------------------------------------------------\n",
      "1          2530.0593       193.7232        2723.7825      \n",
      "1          2530.0593       193.7232        2723.7825      \n",
      "3          1381.7954       748.9352        2130.7307      \n",
      "3          1381.7954       748.9352        2130.7307      \n",
      "5          845.9926        991.4796        1837.4723      \n",
      "5          845.9926        991.4796        1837.4723      \n",
      "10         606.1889        1180.8849       1787.0738      \n",
      "10         606.1889        1180.8849       1787.0738      \n",
      "None       615.8563        1170.2882       1786.1445      \n",
      "------------------------------------------------------------\n",
      "None       615.8563        1170.2882       1786.1445      \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define different complexity levels for decision trees\n",
    "max_depths_random = [1, 3, 5, 10, None]\n",
    "\n",
    "# Store results\n",
    "bias_values_random = []\n",
    "variance_values_random = []\n",
    "error_values_random = []\n",
    "\n",
    "# Perform bias-variance decomposition\n",
    "print(\"Random Dataset Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Max Depth':<10} {'Bias²':<15} {'Variance':<15} {'Total Error':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for depth in max_depths_random:\n",
    "    model = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    bias, variance, error, noise = bias_variance_decomp(\n",
    "        model, X_train_random, y_train_random, X_test_random, y_test_random\n",
    "    )\n",
    "    \n",
    "    bias_values_random.append(bias)\n",
    "    variance_values_random.append(variance)\n",
    "    error_values_random.append(error)\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else \"None\"\n",
    "    print(f\"{depth_str:<10} {bias:<15.4f} {variance:<15.4f} {error:<15.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
